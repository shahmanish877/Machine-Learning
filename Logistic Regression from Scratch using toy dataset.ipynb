{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-30T14:27:49.770786Z",
     "start_time": "2024-01-30T14:27:49.700432600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 11\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "        This is the scratch implementation of Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.param = {}\n",
    "        self.m, self.n = X.shape\n",
    "        self.param['W'] = np.random.randn(self.n,1) * 0.001\n",
    "        self.param['b'] = np.zeros(1)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.result = pd.DataFrame()\n",
    "\n",
    "    def train(self, alpha = 0.001, epochs = 10):\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch: \", epoch, end=\"\")\n",
    "            z = np.dot(self.X,self.param['W']) + self.param['b']\n",
    "\n",
    "            self.y_pred = self.sigmoid(z)\n",
    "            self.result[0] = self.y\n",
    "\n",
    "            #Update the parameters\n",
    "            self.param['W'] = self.param['W'] - alpha * 1/self.m * np.dot(self.X.transpose(), (self.y_pred - np.reshape(self.y, (self.m, 1))))\n",
    "            self.param['b'] =  self.param['b'] - alpha * 1/self.m * np.sum(self.y_pred - np.reshape(self.y, (self.m, 1)))\n",
    "\n",
    "            self.y_pred = self.sigmoid(np.dot(self.X,self.param['W']) + self.param['b'])\n",
    "            loss = self.loss(self.y, self.y_pred)\n",
    "\n",
    "            self.result[1] = self.y_pred\n",
    "            print(\", loss = \", loss)\n",
    "\n",
    "        print(\"\\nFinal Loss is \", loss)\n",
    "        print(\"Coefficients are: \\n W: {}, b = {}\".format(self.param['W'], self.param['b']))\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y, y_pred):\n",
    "        y_zero_loss =  y.T.dot(np.log(y_pred))\n",
    "        y_one_loss = (1-y).T.dot(np.log(1 - y_pred))\n",
    "\n",
    "        return -np.sum(y_zero_loss + y_one_loss)/len(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0/(1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(np.dot(X,self.param['W']) + self.param['b'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T14:27:49.771785800Z",
     "start_time": "2024-01-30T14:27:49.736974900Z"
    }
   },
   "id": "9110dc3d97c291bf"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def generate_data(no_of_data = 100, no_of_features = 4):\n",
    "    features, target = make_classification(n_samples=no_of_data,\n",
    "                                              n_features=no_of_features,\n",
    "                                              n_classes=2,\n",
    "                                              random_state=SEED)\n",
    "\n",
    "    return features, target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T14:27:49.771785800Z",
     "start_time": "2024-01-30T14:27:49.744268500Z"
    }
   },
   "id": "e415d1a0eb8830a0"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0, loss =  0.6369515532007411\n",
      "Epoch:  1, loss =  0.5938916895497949\n",
      "Epoch:  2, loss =  0.5607616106735372\n",
      "Epoch:  3, loss =  0.5343849052409058\n",
      "Epoch:  4, loss =  0.5127474809149306\n",
      "Epoch:  5, loss =  0.49455704392542293\n",
      "Epoch:  6, loss =  0.4789600311882209\n",
      "Epoch:  7, loss =  0.4653735203317431\n",
      "Epoch:  8, loss =  0.45338620460843615\n",
      "Epoch:  9, loss =  0.4426990131263279\n",
      "Epoch:  10, loss =  0.4330885428806972\n",
      "Epoch:  11, loss =  0.4243838793584189\n",
      "Epoch:  12, loss =  0.41645148115308667\n",
      "Epoch:  13, loss =  0.40918505582204895\n",
      "Epoch:  14, loss =  0.40249860853714226\n",
      "Epoch:  15, loss =  0.3963215591280728\n",
      "Epoch:  16, loss =  0.39059523964723414\n",
      "Epoch:  17, loss =  0.38527033364676805\n",
      "Epoch:  18, loss =  0.3803049709000466\n",
      "Epoch:  19, loss =  0.37566328687850387\n",
      "Epoch:  20, loss =  0.37131431747770954\n",
      "Epoch:  21, loss =  0.3672311394431579\n",
      "Epoch:  22, loss =  0.3633901935320615\n",
      "Epoch:  23, loss =  0.35977074544721727\n",
      "Epoch:  24, loss =  0.35635445196505405\n",
      "Epoch:  25, loss =  0.35312500833324995\n",
      "Epoch:  26, loss =  0.350067859145087\n",
      "Epoch:  27, loss =  0.34716995930081634\n",
      "Epoch:  28, loss =  0.3444195748677555\n",
      "Epoch:  29, loss =  0.34180611600598504\n",
      "Epoch:  30, loss =  0.33931999587829326\n",
      "Epoch:  31, loss =  0.33695251077960436\n",
      "Epoch:  32, loss =  0.33469573772040284\n",
      "Epoch:  33, loss =  0.3325424464641425\n",
      "Epoch:  34, loss =  0.33048602361018187\n",
      "Epoch:  35, loss =  0.32852040677473604\n",
      "Epoch:  36, loss =  0.32664002728433705\n",
      "Epoch:  37, loss =  0.3248397600827238\n",
      "Epoch:  38, loss =  0.32311487978031767\n",
      "Epoch:  39, loss =  0.3214610219585191\n",
      "Epoch:  40, loss =  0.3198741489888679\n",
      "Epoch:  41, loss =  0.31835051974714906\n",
      "Epoch:  42, loss =  0.31688666270058663\n",
      "Epoch:  43, loss =  0.315479351926802\n",
      "Epoch:  44, loss =  0.31412558568970833\n",
      "Epoch:  45, loss =  0.3128225672526743\n",
      "Epoch:  46, loss =  0.31156768765527754\n",
      "Epoch:  47, loss =  0.31035851021847155\n",
      "Epoch:  48, loss =  0.30919275657536754\n",
      "Epoch:  49, loss =  0.3080682940521686\n",
      "Epoch:  50, loss =  0.30698312424696234\n",
      "Epoch:  51, loss =  0.30593537267378923\n",
      "Epoch:  52, loss =  0.3049232793562331\n",
      "Epoch:  53, loss =  0.3039451902691886\n",
      "Epoch:  54, loss =  0.30299954953985314\n",
      "Epoch:  55, loss =  0.3020848923296628\n",
      "Epoch:  56, loss =  0.30119983832812447\n",
      "Epoch:  57, loss =  0.3003430857974988\n",
      "Epoch:  58, loss =  0.2995134061142426\n",
      "Epoch:  59, loss =  0.29870963875918655\n",
      "Epoch:  60, loss =  0.2979306867137156\n",
      "Epoch:  61, loss =  0.29717551222386346\n",
      "Epoch:  62, loss =  0.2964431328983015\n",
      "Epoch:  63, loss =  0.29573261810978635\n",
      "Epoch:  64, loss =  0.2950430856727876\n",
      "Epoch:  65, loss =  0.294373698772807\n",
      "Epoch:  66, loss =  0.29372366312537096\n",
      "Epoch:  67, loss =  0.29309222434486376\n",
      "Epoch:  68, loss =  0.29247866550531937\n",
      "Epoch:  69, loss =  0.2918823048770174\n",
      "Epoch:  70, loss =  0.2913024938242708\n",
      "Epoch:  71, loss =  0.2907386148511748\n",
      "Epoch:  72, loss =  0.2901900797833148\n",
      "Epoch:  73, loss =  0.2896563280745365\n",
      "Epoch:  74, loss =  0.28913682522887163\n",
      "Epoch:  75, loss =  0.28863106132860084\n",
      "Epoch:  76, loss =  0.28813854966023583\n",
      "Epoch:  77, loss =  0.28765882543092447\n",
      "Epoch:  78, loss =  0.2871914445684303\n",
      "Epoch:  79, loss =  0.28673598259842625\n",
      "Epoch:  80, loss =  0.28629203359337313\n",
      "Epoch:  81, loss =  0.28585920918773255\n",
      "Epoch:  82, loss =  0.2854371376547021\n",
      "Epoch:  83, loss =  0.2850254630400511\n",
      "Epoch:  84, loss =  0.28462384434899934\n",
      "Epoch:  85, loss =  0.28423195478240393\n",
      "Epoch:  86, loss =  0.28384948101881735\n",
      "Epoch:  87, loss =  0.283476122539251\n",
      "Epoch:  88, loss =  0.2831115909917257\n",
      "Epoch:  89, loss =  0.2827556095929148\n",
      "Epoch:  90, loss =  0.2824079125643913\n",
      "Epoch:  91, loss =  0.28206824460118246\n",
      "Epoch:  92, loss =  0.2817363603705019\n",
      "Epoch:  93, loss =  0.28141202403869336\n",
      "Epoch:  94, loss =  0.2810950088245603\n",
      "Epoch:  95, loss =  0.28078509657739215\n",
      "Epoch:  96, loss =  0.28048207737811853\n",
      "Epoch:  97, loss =  0.28018574916213584\n",
      "Epoch:  98, loss =  0.2798959173624527\n",
      "Epoch:  99, loss =  0.27961239457189857\n",
      "\n",
      "Final Loss is  0.27961239457189857\n",
      "Coefficients are: \n",
      " W: [[-0.63622246]\n",
      " [ 0.6138087 ]\n",
      " [ 0.47391261]\n",
      " [ 1.14212836]], b = [0.07062205]\n"
     ]
    }
   ],
   "source": [
    "no_of_data = 100000\n",
    "no_of_features = 4\n",
    "X, y = generate_data(no_of_data, no_of_features)\n",
    "alpha = 0.1\n",
    "epochs = 100\n",
    "log_model = LogisticRegression(X, y)\n",
    "log_model.train(alpha, epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T14:27:50.654744Z",
     "start_time": "2024-01-30T14:27:49.751786100Z"
    }
   },
   "id": "f7df4c9504732097"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T14:27:50.654744Z",
     "start_time": "2024-01-30T14:27:50.614454900Z"
    }
   },
   "id": "7e6bf93bb1378e51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
